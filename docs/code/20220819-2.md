---
title: 使用 pytorch 实现 Mnist 数据集分类
lang: en-US
---

近几周在网上大致调研了一些 AI 框架，目前来看 pytorch 的确强于 tensorflow，因此暂时决定选用 pytorch 来实现图像识别部分。上来直接图像识别跨度过大，本周首先对 Mnist 手写数据集进行分类。

Mnist 是手写的阿拉伯数字数据集，其中分为四个子集。

- 训练集文件: train_images_idx3_ubyte_file
- 训练集标签文件: train_labels_idx1_ubyte_file
- 测试集文件: test_images_idx3_ubyte_file
- 测试集标签文件: test_labels_idx1_ubyte_file

每张手写图片的像素为 `(28*28)`，像素点比较少，因此比较适合于初学。

### 数据集解析

数据集解析采用了[MNIST 数据集下载与读取](https://herok.blog.csdn.net/article/details/103324368)的代码。数据读取是成功的，但该代码读取的数据最终测试有一些数据格式问题，暂且还没有想明白原因

- 标签文件的数据应该为 int64 格式，读取格式为 float64

```py
y_train = y_train.astype(np.int64)
y_valid = y_valid.astype(np.int64)
```

- 数据文件读取格式为 float64，后续搭建网络要求为 double

```py
torch.set_default_tensor_type(torch.DoubleTensor)
```

- 数据文件为二维格式即 `(28*28)`，平铺为 (784,)

### 转化为 tensor 数据

pytorch 中要求数据类型应该为 tensor

```py
import torch
x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))
```

然后利用 DataLoader 转化数据，DataLoader 部分应该就是后续融合算法重点需要研究的。

> 这里先暂时使用 TensorDataset

```py
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
train_ds = TensorDataset(x_train, y_train)
valid_ds = TensorDataset(x_valid, y_valid)

def get_data(train_ds, valid_ds, bs):
    return (
        DataLoader(train_ds, batch_size=64, shuffle=True),
        DataLoader(valid_ds, batch_size=128)
    )
```

### 模型创建

先初步设计一个比较简单的三层神经网络模型，输出值有 0-9 10 个，因此最终输出层为 10。输入值为 (60000,784)

第一层: (784, 128)
第二层: (128, 256)
第三层: (256, 10)

```py
import torch.nn.functional as F
from torch import nn
loss_func = F.cross_entropy

def model(xb):
    return xb.mm(weights) + bias
from torch import nn
class Mnist_NN(nn.Module):
    def __init__(self):
        super().__init__()
        # 隐层 1
        self.hidden1 = nn.Linear(784,128)
        self.hidden2 = nn.Linear(128,256)
        self.out = nn.Linear(256,10)
        # dropout
        self.dropout = nn.Dropout(0.5)
    def forward(self, x):
        x = F.relu(self.hidden1(x))
        x = self.dropout(x)
        x = F.relu(self.hidden2(x))
        x = self.dropout(x)
        x = self.out(x)
        return x
```

### 创建优化器

一般优化器会有两个选择: SGD Adam，这里使用 Adam，两者的区别还没有完全区分好，详情参考: [优化算法 SGD 与 Adam](https://blog.csdn.net/dbdxwyl/article/details/122209565)

```py
from torch import optim
def get_model():
    model = Mnist_NN()
    # lr 学习率
    # 更新全部参数
    return model, optim.Adam(model.parameters(), lr=0.001)
```

### 定义迭代函数

```py
def loss_batch(model,loss_func, xb, yb, opt=None):
    loss = loss_func(model(xb), yb)
    if opt is not None:
        loss.backward()
        opt.step()
        # 很重要，pytorch 默认累计梯度
        opt.zero_grad()
    return loss.item(), len(xb)
```

### fit 函数

```py
# steps 迭代次数
def fit(steps, model,loss_func, opt, train_dl, valid_dl):
    for step in range(steps):
        # 训练模式 更新参数
        model.train()
        for xb, yb in train_dl:
            loss_batch(model, loss_func, xb, yb, opt)
        # 验证
        model.eval()
        with torch.no_grad():
            # zip
            losses, nums = zip(
                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]
            )
        # 计算平均损失
        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)
        print("当前 step"+str(step), "验证集损失"+str(val_loss))
```

### 分类效果

```py
train_dl, valid_dl = get_data(train_ds, valid_ds, 64)
model,opt = get_model()
fit(100, model, loss_func, opt, train_dl, valid_dl)
```

分类效果并不算好，下周尝试优化一下。
