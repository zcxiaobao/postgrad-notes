<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>使用 pytorch 实现 Mnist 数据集分类 | 文献阅读记录</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/postgrad-notes/assets/css/0.styles.b59f8fc6.css" as="style"><link rel="preload" href="/postgrad-notes/assets/js/app.d9e8d222.js" as="script"><link rel="preload" href="/postgrad-notes/assets/js/2.67cde8bb.js" as="script"><link rel="preload" href="/postgrad-notes/assets/js/22.227e88e6.js" as="script"><link rel="prefetch" href="/postgrad-notes/assets/js/10.d169b7dc.js"><link rel="prefetch" href="/postgrad-notes/assets/js/11.9be4650c.js"><link rel="prefetch" href="/postgrad-notes/assets/js/12.1321f2e1.js"><link rel="prefetch" href="/postgrad-notes/assets/js/13.8d46a8f6.js"><link rel="prefetch" href="/postgrad-notes/assets/js/14.81f98225.js"><link rel="prefetch" href="/postgrad-notes/assets/js/15.d5b3524e.js"><link rel="prefetch" href="/postgrad-notes/assets/js/16.b59fa900.js"><link rel="prefetch" href="/postgrad-notes/assets/js/17.f2ebfa62.js"><link rel="prefetch" href="/postgrad-notes/assets/js/18.5ec43ff9.js"><link rel="prefetch" href="/postgrad-notes/assets/js/19.4ee7e519.js"><link rel="prefetch" href="/postgrad-notes/assets/js/20.5e506c61.js"><link rel="prefetch" href="/postgrad-notes/assets/js/21.d2246565.js"><link rel="prefetch" href="/postgrad-notes/assets/js/23.8088a8d8.js"><link rel="prefetch" href="/postgrad-notes/assets/js/24.19f85afe.js"><link rel="prefetch" href="/postgrad-notes/assets/js/25.3ceddfb2.js"><link rel="prefetch" href="/postgrad-notes/assets/js/26.d1705070.js"><link rel="prefetch" href="/postgrad-notes/assets/js/27.b7d02e9b.js"><link rel="prefetch" href="/postgrad-notes/assets/js/28.22ba9396.js"><link rel="prefetch" href="/postgrad-notes/assets/js/29.0d5768cd.js"><link rel="prefetch" href="/postgrad-notes/assets/js/3.6ea6e7f3.js"><link rel="prefetch" href="/postgrad-notes/assets/js/30.a9cf5e0c.js"><link rel="prefetch" href="/postgrad-notes/assets/js/31.f9717338.js"><link rel="prefetch" href="/postgrad-notes/assets/js/32.bcae8486.js"><link rel="prefetch" href="/postgrad-notes/assets/js/33.3be8b547.js"><link rel="prefetch" href="/postgrad-notes/assets/js/34.802d042e.js"><link rel="prefetch" href="/postgrad-notes/assets/js/35.a4547d84.js"><link rel="prefetch" href="/postgrad-notes/assets/js/36.a579241d.js"><link rel="prefetch" href="/postgrad-notes/assets/js/37.d6bae490.js"><link rel="prefetch" href="/postgrad-notes/assets/js/38.6573b315.js"><link rel="prefetch" href="/postgrad-notes/assets/js/39.040ca776.js"><link rel="prefetch" href="/postgrad-notes/assets/js/4.ea783967.js"><link rel="prefetch" href="/postgrad-notes/assets/js/40.c18b251e.js"><link rel="prefetch" href="/postgrad-notes/assets/js/41.4e788045.js"><link rel="prefetch" href="/postgrad-notes/assets/js/42.859f4c79.js"><link rel="prefetch" href="/postgrad-notes/assets/js/43.b7733a6d.js"><link rel="prefetch" href="/postgrad-notes/assets/js/44.315ced3f.js"><link rel="prefetch" href="/postgrad-notes/assets/js/45.900e5a6f.js"><link rel="prefetch" href="/postgrad-notes/assets/js/46.197793d1.js"><link rel="prefetch" href="/postgrad-notes/assets/js/47.1f8f83ae.js"><link rel="prefetch" href="/postgrad-notes/assets/js/48.3f3fad8d.js"><link rel="prefetch" href="/postgrad-notes/assets/js/49.a17e9efb.js"><link rel="prefetch" href="/postgrad-notes/assets/js/5.50b3e72d.js"><link rel="prefetch" href="/postgrad-notes/assets/js/50.43a9d53c.js"><link rel="prefetch" href="/postgrad-notes/assets/js/51.cd4507c4.js"><link rel="prefetch" href="/postgrad-notes/assets/js/52.156e9895.js"><link rel="prefetch" href="/postgrad-notes/assets/js/53.2c52a486.js"><link rel="prefetch" href="/postgrad-notes/assets/js/54.79cdee37.js"><link rel="prefetch" href="/postgrad-notes/assets/js/55.fb41b7e8.js"><link rel="prefetch" href="/postgrad-notes/assets/js/56.7d95986c.js"><link rel="prefetch" href="/postgrad-notes/assets/js/57.3fa1fa92.js"><link rel="prefetch" href="/postgrad-notes/assets/js/58.cea84511.js"><link rel="prefetch" href="/postgrad-notes/assets/js/59.df4c3e6e.js"><link rel="prefetch" href="/postgrad-notes/assets/js/6.01b7bd81.js"><link rel="prefetch" href="/postgrad-notes/assets/js/60.f6379e59.js"><link rel="prefetch" href="/postgrad-notes/assets/js/61.9960a852.js"><link rel="prefetch" href="/postgrad-notes/assets/js/62.78dceed9.js"><link rel="prefetch" href="/postgrad-notes/assets/js/63.1b4f2bd3.js"><link rel="prefetch" href="/postgrad-notes/assets/js/64.5d73c125.js"><link rel="prefetch" href="/postgrad-notes/assets/js/65.f7ea2f81.js"><link rel="prefetch" href="/postgrad-notes/assets/js/66.bd64b40e.js"><link rel="prefetch" href="/postgrad-notes/assets/js/67.d98cf516.js"><link rel="prefetch" href="/postgrad-notes/assets/js/68.c27dfc63.js"><link rel="prefetch" href="/postgrad-notes/assets/js/69.b5b926ef.js"><link rel="prefetch" href="/postgrad-notes/assets/js/7.115ead7b.js"><link rel="prefetch" href="/postgrad-notes/assets/js/70.4fae42cf.js"><link rel="prefetch" href="/postgrad-notes/assets/js/71.2340033a.js"><link rel="prefetch" href="/postgrad-notes/assets/js/72.3a686a8f.js"><link rel="prefetch" href="/postgrad-notes/assets/js/73.76dac154.js"><link rel="prefetch" href="/postgrad-notes/assets/js/74.d0a60976.js"><link rel="prefetch" href="/postgrad-notes/assets/js/75.7a8280ed.js"><link rel="prefetch" href="/postgrad-notes/assets/js/76.5e66c118.js"><link rel="prefetch" href="/postgrad-notes/assets/js/77.2ce9f0c2.js"><link rel="prefetch" href="/postgrad-notes/assets/js/8.ebdfe4aa.js"><link rel="prefetch" href="/postgrad-notes/assets/js/9.32319c00.js">
    <link rel="stylesheet" href="/postgrad-notes/assets/css/0.styles.b59f8fc6.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/postgrad-notes/" class="home-link router-link-active"><!----> <span class="site-name">文献阅读记录</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/postgrad-notes/chinese/" class="nav-link">
  中文文献
</a></div><div class="nav-item"><a href="/postgrad-notes/english/" class="nav-link">
  外文文献
</a></div><div class="nav-item"><a href="/postgrad-notes/translate/" class="nav-link">
  外文翻译
</a></div><div class="nav-item"><a href="/postgrad-notes/vocabulary/" class="nav-link">
  词汇积累
</a></div><div class="nav-item"><a href="/postgrad-notes/knowledge/" class="nav-link">
  知识积累
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/postgrad-notes/chinese/" class="nav-link">
  中文文献
</a></div><div class="nav-item"><a href="/postgrad-notes/english/" class="nav-link">
  外文文献
</a></div><div class="nav-item"><a href="/postgrad-notes/translate/" class="nav-link">
  外文翻译
</a></div><div class="nav-item"><a href="/postgrad-notes/vocabulary/" class="nav-link">
  词汇积累
</a></div><div class="nav-item"><a href="/postgrad-notes/knowledge/" class="nav-link">
  知识积累
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span></span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/postgrad-notes/code/20220818.html" class="sidebar-link">pytorch 安装遇到的一万个问题</a></li><li><a href="/postgrad-notes/code/20220819.html" class="sidebar-link">Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph an  Convolutional Neural Networks 代码阅读</a></li><li><a href="/postgrad-notes/code/20220819-2.html" aria-current="page" class="active sidebar-link">使用 pytorch 实现 Mnist 数据集分类</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/postgrad-notes/code/20220909.html" class="sidebar-link">NILM 识别代码编写 —— 图像融合</a></li><li><a href="/postgrad-notes/code/20220916.html" class="sidebar-link">20220910-20220916 工作进度</a></li><li><a href="/postgrad-notes/code/20220923.html" class="sidebar-link">20220917-20220923 工作进度</a></li><li><a href="/postgrad-notes/code/20220930.html" class="sidebar-link">20220924-20220930 工作进度</a></li><li><a href="/postgrad-notes/code/20221007.html" class="sidebar-link">20221001-20221007 工作进度</a></li><li><a href="/postgrad-notes/code/20221021.html" class="sidebar-link">20221015-20221021 工作进度</a></li><li><a href="/postgrad-notes/code/20221028.html" class="sidebar-link">20221022-20221028 工作进度</a></li><li><a href="/postgrad-notes/code/20221105.html" class="sidebar-link">20221029-20221105 工作进度</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p>近几周在网上大致调研了一些 AI 框架，目前来看 pytorch 的确强于 tensorflow，因此暂时决定选用 pytorch 来实现图像识别部分。上来直接图像识别跨度过大，本周首先对 Mnist 手写数据集进行分类。</p> <p>Mnist 是手写的阿拉伯数字数据集，其中分为四个子集。</p> <ul><li>训练集文件: train_images_idx3_ubyte_file</li> <li>训练集标签文件: train_labels_idx1_ubyte_file</li> <li>测试集文件: test_images_idx3_ubyte_file</li> <li>测试集标签文件: test_labels_idx1_ubyte_file</li></ul> <p>每张手写图片的像素为 <code>(28*28)</code>，像素点比较少，因此比较适合于初学。</p> <h3 id="数据集解析"><a href="#数据集解析" class="header-anchor">#</a> 数据集解析</h3> <p>数据集解析采用了<a href="https://herok.blog.csdn.net/article/details/103324368" target="_blank" rel="noopener noreferrer">MNIST 数据集下载与读取<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>的代码。数据读取是成功的，但该代码读取的数据最终测试有一些数据格式问题，暂且还没有想明白原因</p> <ul><li>标签文件的数据应该为 int64 格式，读取格式为 float64</li></ul> <div class="language-py extra-class"><pre class="language-py"><code>y_train <span class="token operator">=</span> y_train<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>
y_valid <span class="token operator">=</span> y_valid<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>
</code></pre></div><ul><li>数据文件读取格式为 float64，后续搭建网络要求为 double</li></ul> <div class="language-py extra-class"><pre class="language-py"><code>torch<span class="token punctuation">.</span>set_default_tensor_type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>DoubleTensor<span class="token punctuation">)</span>
</code></pre></div><ul><li>数据文件为二维格式即 <code>(28*28)</code>，平铺为 (784,)</li></ul> <h3 id="转化为-tensor-数据"><a href="#转化为-tensor-数据" class="header-anchor">#</a> 转化为 tensor 数据</h3> <p>pytorch 中要求数据类型应该为 tensor</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_valid<span class="token punctuation">,</span>y_valid <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_valid<span class="token punctuation">,</span>y_valid<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>然后利用 DataLoader 转化数据，DataLoader 部分应该就是后续融合算法重点需要研究的。</p> <blockquote><p>这里先暂时使用 TensorDataset</p></blockquote> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
valid_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
</code></pre></div><h3 id="模型创建"><a href="#模型创建" class="header-anchor">#</a> 模型创建</h3> <p>先初步设计一个比较简单的三层神经网络模型，输出值有 0-9 10 个，因此最终输出层为 10。输入值为 (60000,784)</p> <p>第一层: (784, 128)
第二层: (128, 256)
第三层: (256, 10)</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
loss_func <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy

<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> xb<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weights<span class="token punctuation">)</span> <span class="token operator">+</span> bias
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">class</span> <span class="token class-name">Mnist_NN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 隐层 1</span>
        self<span class="token punctuation">.</span>hidden1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token comment"># dropout</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre></div><h3 id="创建优化器"><a href="#创建优化器" class="header-anchor">#</a> 创建优化器</h3> <p>一般优化器会有两个选择: SGD Adam，这里使用 Adam，两者的区别还没有完全区分好，详情参考: <a href="https://blog.csdn.net/dbdxwyl/article/details/122209565" target="_blank" rel="noopener noreferrer">优化算法 SGD 与 Adam<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
<span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> Mnist_NN<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># lr 学习率</span>
    <span class="token comment"># 更新全部参数</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="定义迭代函数"><a href="#定义迭代函数" class="header-anchor">#</a> 定义迭代函数</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">loss_batch</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span>
    <span class="token keyword">if</span> opt <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 很重要，pytorch 默认累计梯度</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre></div><h3 id="fit-函数"><a href="#fit-函数" class="header-anchor">#</a> fit 函数</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># steps 迭代次数</span>
<span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>steps<span class="token punctuation">,</span> model<span class="token punctuation">,</span>loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 训练模式 更新参数</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
            loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token punctuation">)</span>
        <span class="token comment"># 验证</span>
        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># zip</span>
            losses<span class="token punctuation">,</span> nums <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
                <span class="token operator">*</span><span class="token punctuation">[</span>loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">]</span>
            <span class="token punctuation">)</span>
        <span class="token comment"># 计算平均损失</span>
        val_loss <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>losses<span class="token punctuation">,</span>nums<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;当前 step&quot;</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>step<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&quot;验证集损失&quot;</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="分类效果"><a href="#分类效果" class="header-anchor">#</a> 分类效果</h3> <div class="language-py extra-class"><pre class="language-py"><code>train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
model<span class="token punctuation">,</span>opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
fit<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>
</code></pre></div><p>分类效果并不算好，下周尝试优化一下。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/postgrad-notes/code/20220819.html" class="prev">
        Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph an  Convolutional Neural Networks 代码阅读
      </a></span> <span class="next"><a href="/postgrad-notes/code/20220909.html">
        NILM 识别代码编写 —— 图像融合
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/postgrad-notes/assets/js/app.d9e8d222.js" defer></script><script src="/postgrad-notes/assets/js/2.67cde8bb.js" defer></script><script src="/postgrad-notes/assets/js/22.227e88e6.js" defer></script>
  </body>
</html>
